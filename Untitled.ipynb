{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16 = models.vgg16(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(vgg16.features.children()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class StyleTransferNet():\n",
    "    def __init__(self):\n",
    "        vgg16 = models.vgg16(pretrained=True)\n",
    "        feature_maps = list(vgg16.features.children())\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=2,stride=2)\n",
    "        self.block_1 = nn.Sequential(\n",
    "            feature_maps[0],\n",
    "            feature_maps[1],\n",
    "            feature_maps[2],\n",
    "            feature_maps[3],\n",
    "        )\n",
    "        \n",
    "        self.block_2 = nn.Sequential(\n",
    "            feature_maps[5],\n",
    "            feature_maps[6],\n",
    "            feature_maps[7],\n",
    "            feature_maps[8],\n",
    "        )\n",
    "        \n",
    "        self.block_3 = nn.Sequential(\n",
    "            feature_maps[10],\n",
    "            feature_maps[11],\n",
    "            feature_maps[12],\n",
    "            feature_maps[13],\n",
    "            feature_maps[14],\n",
    "            feature_maps[15],\n",
    "        )\n",
    "        \n",
    "        self.block_4 = nn.Sequential(\n",
    "            feature_maps[17],\n",
    "            feature_maps[18],\n",
    "            feature_maps[19],\n",
    "            feature_maps[20],\n",
    "            feature_maps[21],\n",
    "            feature_maps[22],\n",
    "        )\n",
    "        \n",
    "        self.block_5 = nn.Sequential(\n",
    "            feature_maps[24],\n",
    "            feature_maps[25],\n",
    "            feature_maps[26],\n",
    "            feature_maps[27],\n",
    "            feature_maps[28],\n",
    "            feature_maps[29],\n",
    "        )\n",
    "        \n",
    "    def get_features(self,x):\n",
    "        block_1_features = self.block_1(x)\n",
    "        x = self.avgpool(block_1_features)\n",
    "        block_2_features = self.block_2(x)\n",
    "        x = self.avgpool(block_2_features)\n",
    "        block_3_features = self.block_3(x)\n",
    "        x = self.avgpool(block_3_features)\n",
    "        block_4_features = self.block_4(x)\n",
    "        x = self.avgpool(block_4_features)\n",
    "        block_5_features = self.block_5(x)\n",
    "        return [block_1_features,block_2_features,block_3_features,block_4_features,block_5_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = StyleTransferNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img = scipy.misc.imread(\"city.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = w * 1.0 / img.shape[1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img = scipy.misc.imresize(img, (int(img.shape[0] * r),(int(img.shape[1]*r))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = img.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sub_mean(img):\n",
    "    for i in range(3):\n",
    "        img[:,:,i] -= mean[i]\n",
    "        \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean = np.array([103.939, 116.779, 123.68])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img = sub_mean(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(288, 500, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tran = transforms.Compose([transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img = tran(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img = img.resize_(1,img.size()[0],img.size()[1],img.size()[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "content = torch.autograd.Variable(img,requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "content_features = net.get_features(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "style = scipy.misc.imread(\"style.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = w * 1.0 / style.shape[1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "style = scipy.misc.imresize(style, (int(style.shape[0] * r),(int(style.shape[1]*r))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "style = style.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "style = sub_mean(style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "style = tran(style)\n",
    "style = style.resize_(1,style.size()[0],style.size()[1],style.size()[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "style = torch.autograd.Variable(style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "style_features = net.get_features(style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentLoss(nn.Module):\n",
    "    def __init__(self, weights):\n",
    "        super(ContentLoss,self).__init__()\n",
    "        self.weights = weights\n",
    "        \n",
    "    def forward(self, preds, targets):\n",
    "        self.loss = 0\n",
    "        for weight, pred, target in zip(self.weights, preds, targets):\n",
    "            self.loss += torch.mean((pred - target) ** 2) * weight\n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, retain_graph=True):\n",
    "        self.loss.backward(retain_graph=retain_graph)\n",
    "        return self.loss\n",
    "    \n",
    "class GramMatrix(nn.Module):\n",
    "    def forward(self, feature_map):\n",
    "        a,b,c,d = feature_map.size()\n",
    "        feature = feature_map.view(a*b, c*d)\n",
    "        matrix = torch.mm(feature, feature.t())\n",
    "        \n",
    "        return matrix.div(a*b*c*d)\n",
    "    \n",
    "class StyleLoss(nn.Module):\n",
    "    def __init__(self, weights):\n",
    "        super(StyleLoss, self).__init__()\n",
    "        self.weights = weights\n",
    "        self.gram = GramMatrix()\n",
    "        \n",
    "    def forward(self, preds, targets):\n",
    "        self.loss = 0\n",
    "        for weight, pred, target in zip(self.weights, preds, targets):\n",
    "            g_pred = self.gram(pred)\n",
    "            g_target = self.gram(target)\n",
    "            self.loss += torch.mean((g_pred - g_target) ** 2) * weight\n",
    "            \n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, retain_graph=True):\n",
    "        self.loss.backward(retain_graph = retain_graph)\n",
    "        return self.loss\n",
    "    \n",
    "class TotalLoss(nn.Module):\n",
    "    def __init__(self, content, style):\n",
    "        super(TotalLoss, self).__init__()\n",
    "        self.content_loss = content\n",
    "        self.style_loss = style\n",
    "        \n",
    "    def forward(self, gens, contents, styles):\n",
    "        self.loss = self.content_loss(gens,contents) + self.style_loss(gens,styles)\n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, retain_graph=True):\n",
    "        self.loss.backward(retain_graph=retain_graph)\n",
    "        return self.closs+self.sloss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = torch.autograd.Variable(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_features = net.get_features(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "content_loss = ContentLoss([0.5,1,1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = content_loss(gen_features,content_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "style_loss = StyleLoss([4000,8000,800,800,800])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_style = style_loss(gen_features, style_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "1.00000e-05 *\n",
       "  3.5181\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = TotalLoss(content_loss,style_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_func(gen_features,content_features,style_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_param = nn.Parameter(gen.data)\n",
    "opt = torch.optim.LBFGS([gen_param])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "moded = nn.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "1.00000e-02 *\n",
      "  1.0113\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      "1.00000e-02 *\n",
      "  1.0113\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      "1.00000e-02 *\n",
      "  1.0112\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      "1.00000e-02 *\n",
      "  1.0112\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      "1.00000e-02 *\n",
      "  1.0112\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      "inf\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _ in range(0,2):\n",
    "    def closure():\n",
    "        opt.zero_grad()\n",
    "        gen_features = net.get_features(gen_param)\n",
    "        loss = loss_func(gen_features,content_features,style_features)\n",
    "        loss.backward(retain_graph=True)\n",
    "        print(loss)\n",
    "        return loss\n",
    "    opt.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "( 0 , 0 ,.,.) = \n",
       "  2.4535e+12  3.6431e+12  4.2900e+12  ...   6.6597e+12  9.2908e+12  1.1243e+12\n",
       "  5.0289e+12  4.3893e+11 -4.0697e+12  ...   5.4789e+12  1.1203e+13  2.8992e+12\n",
       " -4.6099e+12 -1.5890e+13 -2.1428e+13  ...  -1.9816e+12  5.3796e+12  9.0802e+11\n",
       "                 ...                   ⋱                   ...                \n",
       " -4.0767e+12  4.6032e+12  3.1090e+13  ...  -2.9316e+12  8.4038e+12  6.0565e+12\n",
       " -9.5824e+12 -9.2737e+12  3.4565e+12  ...   2.7708e+12  6.6660e+12  2.9354e+12\n",
       " -5.0812e+12 -7.8063e+12 -5.4116e+12  ...  -4.7237e+12 -1.7901e+12 -2.0716e+12\n",
       "\n",
       "( 0 , 1 ,.,.) = \n",
       " -5.4878e+12 -6.3358e+12 -4.1210e+12  ...  -2.8581e+12 -4.2451e+12 -8.6305e+12\n",
       " -4.9304e+11 -3.0907e+12 -2.6643e+12  ...   2.6028e+12 -3.2325e+12 -9.6822e+12\n",
       " -6.9339e+12 -8.5107e+12 -1.6484e+12  ...   1.7408e+13  3.6198e+12 -7.0220e+12\n",
       "                 ...                   ⋱                   ...                \n",
       "  1.0108e+13 -4.3965e+12 -2.6214e+13  ...  -3.1285e+13 -2.1247e+13 -3.1646e+12\n",
       "  7.3736e+12 -7.8989e+11 -1.7695e+13  ...   8.8252e+12  4.3310e+12  4.4960e+12\n",
       "  7.3526e+12  6.1450e+12 -1.9344e+12  ...   4.8733e+12  2.0945e+12  8.7473e+11\n",
       "\n",
       "( 0 , 2 ,.,.) = \n",
       " -7.4666e+12 -9.4304e+12 -8.5306e+12  ...  -2.3214e+12 -4.2312e+12 -4.0096e+12\n",
       " -6.1745e+12 -5.8299e+12 -6.0646e+11  ...   2.9496e+12 -4.0713e+12 -4.7432e+12\n",
       " -5.2773e+12  2.0950e+12  1.7171e+13  ...   1.5756e+13  2.1732e+12 -3.4576e+12\n",
       "                 ...                   ⋱                   ...                \n",
       "  1.0068e+13 -3.5825e+12 -3.1090e+13  ...  -1.0858e+13 -9.5900e+12  3.0602e+12\n",
       "  1.1994e+13  1.0419e+13 -4.2630e+12  ...   1.0269e+13  5.2119e+12  6.3426e+12\n",
       "  8.4660e+12  1.2109e+13  8.8051e+12  ...   1.0708e+13  7.9215e+12  5.5485e+12\n",
       "[torch.FloatTensor of size 1x3x288x500]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "( 0 , 0 ,.,.) = \n",
       "  2.4535e+12  3.6431e+12  4.2900e+12  ...   6.6597e+12  9.2908e+12  1.1243e+12\n",
       "  5.0289e+12  4.3893e+11 -4.0697e+12  ...   5.4789e+12  1.1203e+13  2.8992e+12\n",
       " -4.6099e+12 -1.5890e+13 -2.1428e+13  ...  -1.9816e+12  5.3796e+12  9.0802e+11\n",
       "                 ...                   ⋱                   ...                \n",
       " -4.0767e+12  4.6032e+12  3.1090e+13  ...  -2.9316e+12  8.4038e+12  6.0565e+12\n",
       " -9.5824e+12 -9.2737e+12  3.4565e+12  ...   2.7708e+12  6.6660e+12  2.9354e+12\n",
       " -5.0812e+12 -7.8063e+12 -5.4116e+12  ...  -4.7237e+12 -1.7901e+12 -2.0716e+12\n",
       "\n",
       "( 0 , 1 ,.,.) = \n",
       " -5.4878e+12 -6.3358e+12 -4.1210e+12  ...  -2.8581e+12 -4.2451e+12 -8.6305e+12\n",
       " -4.9304e+11 -3.0907e+12 -2.6643e+12  ...   2.6028e+12 -3.2325e+12 -9.6822e+12\n",
       " -6.9339e+12 -8.5107e+12 -1.6484e+12  ...   1.7408e+13  3.6198e+12 -7.0220e+12\n",
       "                 ...                   ⋱                   ...                \n",
       "  1.0108e+13 -4.3965e+12 -2.6214e+13  ...  -3.1285e+13 -2.1247e+13 -3.1646e+12\n",
       "  7.3736e+12 -7.8989e+11 -1.7695e+13  ...   8.8252e+12  4.3310e+12  4.4960e+12\n",
       "  7.3526e+12  6.1450e+12 -1.9344e+12  ...   4.8733e+12  2.0945e+12  8.7473e+11\n",
       "\n",
       "( 0 , 2 ,.,.) = \n",
       " -7.4666e+12 -9.4304e+12 -8.5306e+12  ...  -2.3214e+12 -4.2312e+12 -4.0096e+12\n",
       " -6.1745e+12 -5.8299e+12 -6.0646e+11  ...   2.9496e+12 -4.0713e+12 -4.7432e+12\n",
       " -5.2773e+12  2.0950e+12  1.7171e+13  ...   1.5756e+13  2.1732e+12 -3.4576e+12\n",
       "                 ...                   ⋱                   ...                \n",
       "  1.0068e+13 -3.5825e+12 -3.1090e+13  ...  -1.0858e+13 -9.5900e+12  3.0602e+12\n",
       "  1.1994e+13  1.0419e+13 -4.2630e+12  ...   1.0269e+13  5.2119e+12  6.3426e+12\n",
       "  8.4660e+12  1.2109e+13  8.8051e+12  ...   1.0708e+13  7.9215e+12  5.5485e+12\n",
       "[torch.FloatTensor of size 1x3x288x500]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean((gen - gen_param) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "( 0 , 0 ,.,.) = \n",
       "  2.4535e+12  3.6431e+12  4.2900e+12  ...   6.6597e+12  9.2908e+12  1.1243e+12\n",
       "  5.0289e+12  4.3893e+11 -4.0697e+12  ...   5.4789e+12  1.1203e+13  2.8992e+12\n",
       " -4.6099e+12 -1.5890e+13 -2.1428e+13  ...  -1.9816e+12  5.3796e+12  9.0802e+11\n",
       "                 ...                   ⋱                   ...                \n",
       " -4.0767e+12  4.6032e+12  3.1090e+13  ...  -2.9316e+12  8.4038e+12  6.0565e+12\n",
       " -9.5824e+12 -9.2737e+12  3.4565e+12  ...   2.7708e+12  6.6660e+12  2.9354e+12\n",
       " -5.0812e+12 -7.8063e+12 -5.4116e+12  ...  -4.7237e+12 -1.7901e+12 -2.0716e+12\n",
       "\n",
       "( 0 , 1 ,.,.) = \n",
       " -5.4878e+12 -6.3358e+12 -4.1210e+12  ...  -2.8581e+12 -4.2451e+12 -8.6305e+12\n",
       " -4.9304e+11 -3.0907e+12 -2.6643e+12  ...   2.6028e+12 -3.2325e+12 -9.6822e+12\n",
       " -6.9339e+12 -8.5107e+12 -1.6484e+12  ...   1.7408e+13  3.6198e+12 -7.0220e+12\n",
       "                 ...                   ⋱                   ...                \n",
       "  1.0108e+13 -4.3965e+12 -2.6214e+13  ...  -3.1285e+13 -2.1247e+13 -3.1646e+12\n",
       "  7.3736e+12 -7.8989e+11 -1.7695e+13  ...   8.8252e+12  4.3310e+12  4.4960e+12\n",
       "  7.3526e+12  6.1450e+12 -1.9344e+12  ...   4.8733e+12  2.0945e+12  8.7473e+11\n",
       "\n",
       "( 0 , 2 ,.,.) = \n",
       " -7.4666e+12 -9.4304e+12 -8.5306e+12  ...  -2.3214e+12 -4.2312e+12 -4.0096e+12\n",
       " -6.1745e+12 -5.8299e+12 -6.0646e+11  ...   2.9496e+12 -4.0713e+12 -4.7432e+12\n",
       " -5.2773e+12  2.0950e+12  1.7171e+13  ...   1.5756e+13  2.1732e+12 -3.4576e+12\n",
       "                 ...                   ⋱                   ...                \n",
       "  1.0068e+13 -3.5825e+12 -3.1090e+13  ...  -1.0858e+13 -9.5900e+12  3.0602e+12\n",
       "  1.1994e+13  1.0419e+13 -4.2630e+12  ...   1.0269e+13  5.2119e+12  6.3426e+12\n",
       "  8.4660e+12  1.2109e+13  8.8051e+12  ...   1.0708e+13  7.9215e+12  5.5485e+12\n",
       "[torch.FloatTensor of size 1x3x288x500]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
